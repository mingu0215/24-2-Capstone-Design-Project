{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["ObN755t0QR6Y"],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-mTGGjb7y8k","outputId":"63209a91-d01c-4e53-eeb0-ddd306a07bb5","executionInfo":{"status":"ok","timestamp":1733585714915,"user_tz":-540,"elapsed":16935,"user":{"displayName":"강민규","userId":"08241585623496994447"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# 경로 설정\n","%cd /content/drive/MyDrive/성균관대학교 과제/3학년 2학기/캡스톤설계프로젝트"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9hqyacsy881e","outputId":"e1a81fe7-2e85-4922-d0e3-c109772422cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/24-2 Capston\n"]}]},{"cell_type":"code","source":["# 라이브러리 로드\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch import Tensor\n","import sklearn\n","import transformers\n","from torch.nn import BCEWithLogitsLoss\n","from torch.utils.data import DataLoader, Dataset, TensorDataset\n","from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n","from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n","import openai\n","from openai import OpenAI\n","from torch.utils.data import TensorDataset\n","from transformers import BertTokenizer\n","from tqdm import tqdm\n","import pickle\n","import math\n","import copy\n","from torch.cuda.amp import autocast, GradScaler\n","\n","#import 추가\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.utils.class_weight import compute_class_weight"],"metadata":{"id":"YXL507xq89Hw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gpu 디바이스 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"kDtBydvt9BiS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##data"],"metadata":{"id":"m_5zR9bS9Cpe"}},{"cell_type":"code","source":["text='안녕하세요. 저는 요즘 먹는 것 때문에 너무 힘들어서 여기에 고민을 털어놓습니다.음식을 보면 한편으로는 너무 먹고 싶다가도, 또 한편으로는 먹으면 안 된다는 생각이 머릿속에서 떠나질 않습니다. 가끔 참지 못하고 폭식하게 되면 이후에는 죄책감에 사로잡혀 아무것도 못 하겠어요. 그러다 보면 또 굶고, 몸이 버티지 못할 정도로 힘들어지는 악순환에 빠집니다. 제 몸무게나 외모에 대한 집착이 너무 커져서, 제 자신을 있는 그대로 받아들이는 게 점점 어려워지고 있어요. 사람들은 \"그냥 조금만 먹으면 되잖아,\" \"왜 그렇게 예민하게 굴어?\"라고 말하지만, 이게 제 마음대로 되는 게 아니란 걸 아무도 몰라줍니다. 혹시 저처럼 이런 고민을 해보신 분들이 있다면, 어떻게 이겨낼 수 있었는지 조언 부탁드립니다. 제발 저도 자유롭게 살고 싶어요.'"],"metadata":{"id":"A3Xke82i9ZCK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###symptom identification"],"metadata":{"id":"bCsbD-PrGYuY"}},{"cell_type":"code","source":["symptoms=['음식 섭취 제한','폭식', '체중과 체형에 대한 집착', '예민함','통제력 부족']\n","mean_uncertainty=[0.059, 0.0541, 0.0614, 0.0569, 0.0547, 0.0572, 0.0478, 0.0625, 0.0471, 0.0478, 0.0526, 0.0498, 0.0502, 0.0595, 0.062, 0.0525, 0.0438, 0.0468, 0.0495, 0.0563, 0.0498, 0.0439, 0.0499, 0.0704, 0.0522, 0.0574, 0.0529, 0.0522, 0.0632, 0.0638, 0.0426, 0.0441, 0.0512, 0.0542, 0.0492, 0.0489, 0.0488, 0.0501, 0.0462, 0.0584, 0.0462, 0.0564, 0.0431, 0.0578, 0.0605, 0.0592, 0.0538, 0.0519]"],"metadata":{"id":"kZx2MGvtGPiR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#GPT로 정의한 mean_uncertainty\n","mean_uncertainty=torch.tensor([0.0585, 0.049, 0.0513, 0.0496, 0.0519, 0.0501, 0.054, 0.0481, 0.0529, 0.0522, 0.0518, 0.0512, 0.0514, 0.0465, 0.0463, 0.0561,\n"," 0.0537, 0.0495, 0.0504, 0.0608, 0.0523, 0.0522, 0.0524, 0.0529, 0.0481, 0.0509, 0.0526, 0.0488, 0.0504, 0.0519, 0.0526, 0.052,\n"," 0.047, 0.0524, 0.049, 0.0555, 0.0493, 0.05, 0.0473, 0.0491, 0.0482, 0.0484, 0.0534, 0.0543, 0.0495, 0.0577, 0.0458, 0.0505])\n","print(mean_uncertainty.shape)\n","\n","#차원 확장 [batch_size, num_labels]\n","mean_uncertainty = mean_uncertainty.unsqueeze(0)\n","mean_uncertainty = mean_uncertainty.expand(64, -1)\n","print(mean_uncertainty.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K_aX2CArVIzn","outputId":"fc9485c6-0652-4bd4-9165-d07971353a3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([48])\n","torch.Size([64, 48])\n"]}]},{"cell_type":"markdown","source":["###context extraction"],"metadata":{"id":"wYns3KFTGeUG"}},{"cell_type":"code","source":["cause=1\n","cause_evidence='사용자는 음식 섭취에 대한 고민과 죄책감, 외모에 대한 집착으로 인해 스트레스를 받고 있으며, 악순환에 빠져 있다고 보고함.'\n","frequency=0\n","freq_evidence='언급 없음'\n","duration=0\n","duration_evidence='언급 없음'\n","age=0\n","age_evidence='언급 없음'\n","social=1\n","social_evidence='사용자는 음식에 대한 집착과 죄책감으로 인해 사회적 관계에서 불편함을 겪고 있음'\n","academic=0\n","academic_evidence='언급 없음'\n","occupational=0\n","occupational_evidence='언급 없음'\n","life_threatening=0\n","life_threatening_evidence='언급 없음'"],"metadata":{"id":"RxkciUmrIMBA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=pd.DataFrame([[text, symptoms, cause, frequency, duration, age, social, academic, occupational, life_threatening,\n","                 cause_evidence, freq_evidence, duration_evidence, age_evidence, social_evidence, academic_evidence, occupational_evidence, life_threatening_evidence]]\n","                , columns=['filtered_sentences','final_symptoms','cause','frequency','duration','age','social','academic','occupational','life-threatening',\n","                           'cause_evidence','freq_evidence','duration_evidence','age_evidence','social_evidence','academic_evidence','occupational_evidence','life-threatening_evidence'])\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"vIeU8jWikSzy","outputId":"786a8e1d-463a-4e91-e2ca-fd37e745e4bf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                  filtered_sentences  \\\n","0  안녕하세요. 저는 요즘 먹는 것 때문에 너무 힘들어서 여기에 고민을 털어놓습니다.음...   \n","\n","                               final_symptoms  cause  frequency  duration  \\\n","0  [음식 섭취 제한, 폭식, 체중과 체형에 대한 집착, 예민함, 통제력 부족]      1          0         0   \n","\n","   age  social  academic  occupational  life-threatening  \\\n","0    0       1         0             0                 0   \n","\n","                                      cause_evidence freq_evidence  \\\n","0  사용자는 음식 섭취에 대한 고민과 죄책감, 외모에 대한 집착으로 인해 스트레스를 받...         언급 없음   \n","\n","  duration_evidence age_evidence  \\\n","0             언급 없음        언급 없음   \n","\n","                                social_evidence academic_evidence  \\\n","0  사용자는 음식에 대한 집착과 죄책감으로 인해 사회적 관계에서 불편함을 겪고 있음             언급 없음   \n","\n","  occupational_evidence life-threatening_evidence  \n","0                 언급 없음                     언급 없음  "],"text/html":["\n","  <div id=\"df-cbae3932-bc70-42ee-909b-e3d256e9a6d8\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filtered_sentences</th>\n","      <th>final_symptoms</th>\n","      <th>cause</th>\n","      <th>frequency</th>\n","      <th>duration</th>\n","      <th>age</th>\n","      <th>social</th>\n","      <th>academic</th>\n","      <th>occupational</th>\n","      <th>life-threatening</th>\n","      <th>cause_evidence</th>\n","      <th>freq_evidence</th>\n","      <th>duration_evidence</th>\n","      <th>age_evidence</th>\n","      <th>social_evidence</th>\n","      <th>academic_evidence</th>\n","      <th>occupational_evidence</th>\n","      <th>life-threatening_evidence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>안녕하세요. 저는 요즘 먹는 것 때문에 너무 힘들어서 여기에 고민을 털어놓습니다.음...</td>\n","      <td>[음식 섭취 제한, 폭식, 체중과 체형에 대한 집착, 예민함, 통제력 부족]</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>사용자는 음식 섭취에 대한 고민과 죄책감, 외모에 대한 집착으로 인해 스트레스를 받...</td>\n","      <td>언급 없음</td>\n","      <td>언급 없음</td>\n","      <td>언급 없음</td>\n","      <td>사용자는 음식에 대한 집착과 죄책감으로 인해 사회적 관계에서 불편함을 겪고 있음</td>\n","      <td>언급 없음</td>\n","      <td>언급 없음</td>\n","      <td>언급 없음</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbae3932-bc70-42ee-909b-e3d256e9a6d8')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cbae3932-bc70-42ee-909b-e3d256e9a6d8 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cbae3932-bc70-42ee-909b-e3d256e9a6d8');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_edc79a9d-a063-4537-8986-d7a9fb87feb1\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_edc79a9d-a063-4537-8986-d7a9fb87feb1 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"filtered_sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\uc548\\ub155\\ud558\\uc138\\uc694. \\uc800\\ub294 \\uc694\\uc998 \\uba39\\ub294 \\uac83 \\ub54c\\ubb38\\uc5d0 \\ub108\\ubb34 \\ud798\\ub4e4\\uc5b4\\uc11c \\uc5ec\\uae30\\uc5d0 \\uace0\\ubbfc\\uc744 \\ud138\\uc5b4\\ub193\\uc2b5\\ub2c8\\ub2e4.\\uc74c\\uc2dd\\uc744 \\ubcf4\\uba74 \\ud55c\\ud3b8\\uc73c\\ub85c\\ub294 \\ub108\\ubb34 \\uba39\\uace0 \\uc2f6\\ub2e4\\uac00\\ub3c4, \\ub610 \\ud55c\\ud3b8\\uc73c\\ub85c\\ub294 \\uba39\\uc73c\\uba74 \\uc548 \\ub41c\\ub2e4\\ub294 \\uc0dd\\uac01\\uc774 \\uba38\\ub9bf\\uc18d\\uc5d0\\uc11c \\ub5a0\\ub098\\uc9c8 \\uc54a\\uc2b5\\ub2c8\\ub2e4. \\uac00\\ub054 \\ucc38\\uc9c0 \\ubabb\\ud558\\uace0 \\ud3ed\\uc2dd\\ud558\\uac8c \\ub418\\uba74 \\uc774\\ud6c4\\uc5d0\\ub294 \\uc8c4\\ucc45\\uac10\\uc5d0 \\uc0ac\\ub85c\\uc7a1\\ud600 \\uc544\\ubb34\\uac83\\ub3c4 \\ubabb \\ud558\\uaca0\\uc5b4\\uc694. \\uadf8\\ub7ec\\ub2e4 \\ubcf4\\uba74 \\ub610 \\uad76\\uace0, \\ubab8\\uc774 \\ubc84\\ud2f0\\uc9c0 \\ubabb\\ud560 \\uc815\\ub3c4\\ub85c \\ud798\\ub4e4\\uc5b4\\uc9c0\\ub294 \\uc545\\uc21c\\ud658\\uc5d0 \\ube60\\uc9d1\\ub2c8\\ub2e4. \\uc81c \\ubab8\\ubb34\\uac8c\\ub098 \\uc678\\ubaa8\\uc5d0 \\ub300\\ud55c \\uc9d1\\ucc29\\uc774 \\ub108\\ubb34 \\ucee4\\uc838\\uc11c, \\uc81c \\uc790\\uc2e0\\uc744 \\uc788\\ub294 \\uadf8\\ub300\\ub85c \\ubc1b\\uc544\\ub4e4\\uc774\\ub294 \\uac8c \\uc810\\uc810 \\uc5b4\\ub824\\uc6cc\\uc9c0\\uace0 \\uc788\\uc5b4\\uc694. \\uc0ac\\ub78c\\ub4e4\\uc740 \\\"\\uadf8\\ub0e5 \\uc870\\uae08\\ub9cc \\uba39\\uc73c\\uba74 \\ub418\\uc796\\uc544,\\\" \\\"\\uc65c \\uadf8\\ub807\\uac8c \\uc608\\ubbfc\\ud558\\uac8c \\uad74\\uc5b4?\\\"\\ub77c\\uace0 \\ub9d0\\ud558\\uc9c0\\ub9cc, \\uc774\\uac8c \\uc81c \\ub9c8\\uc74c\\ub300\\ub85c \\ub418\\ub294 \\uac8c \\uc544\\ub2c8\\ub780 \\uac78 \\uc544\\ubb34\\ub3c4 \\ubab0\\ub77c\\uc90d\\ub2c8\\ub2e4. \\ud639\\uc2dc \\uc800\\ucc98\\ub7fc \\uc774\\ub7f0 \\uace0\\ubbfc\\uc744 \\ud574\\ubcf4\\uc2e0 \\ubd84\\ub4e4\\uc774 \\uc788\\ub2e4\\uba74, \\uc5b4\\ub5bb\\uac8c \\uc774\\uaca8\\ub0bc \\uc218 \\uc788\\uc5c8\\ub294\\uc9c0 \\uc870\\uc5b8 \\ubd80\\ud0c1\\ub4dc\\ub9bd\\ub2c8\\ub2e4. \\uc81c\\ubc1c \\uc800\\ub3c4 \\uc790\\uc720\\ub86d\\uac8c \\uc0b4\\uace0 \\uc2f6\\uc5b4\\uc694.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"final_symptoms\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cause\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"frequency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"social\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"academic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"occupational\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"life-threatening\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cause_evidence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\uc0ac\\uc6a9\\uc790\\ub294 \\uc74c\\uc2dd \\uc12d\\ucde8\\uc5d0 \\ub300\\ud55c \\uace0\\ubbfc\\uacfc \\uc8c4\\ucc45\\uac10, \\uc678\\ubaa8\\uc5d0 \\ub300\\ud55c \\uc9d1\\ucc29\\uc73c\\ub85c \\uc778\\ud574 \\uc2a4\\ud2b8\\ub808\\uc2a4\\ub97c \\ubc1b\\uace0 \\uc788\\uc73c\\uba70, \\uc545\\uc21c\\ud658\\uc5d0 \\ube60\\uc838 \\uc788\\ub2e4\\uace0 \\ubcf4\\uace0\\ud568.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"freq_evidence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\uc5b8\\uae09 \\uc5c6\\uc74c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration_evidence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\uc5b8\\uae09 \\uc5c6\\uc74c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age_evidence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\uc5b8\\uae09 \\uc5c6\\uc74c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"social_evidence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\uc0ac\\uc6a9\\uc790\\ub294 \\uc74c\\uc2dd\\uc5d0 \\ub300\\ud55c \\uc9d1\\ucc29\\uacfc \\uc8c4\\ucc45\\uac10\\uc73c\\ub85c \\uc778\\ud574 \\uc0ac\\ud68c\\uc801 \\uad00\\uacc4\\uc5d0\\uc11c \\ubd88\\ud3b8\\ud568\\uc744 \\uacaa\\uace0 \\uc788\\uc74c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"academic_evidence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\uc5b8\\uae09 \\uc5c6\\uc74c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"occupational_evidence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\uc5b8\\uae09 \\uc5c6\\uc74c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"life-threatening_evidence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\uc5b8\\uae09 \\uc5c6\\uc74c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["##model class"],"metadata":{"id":"ObN755t0QR6Y"}},{"cell_type":"code","source":["# PsyEx (only risky-post stream) 모델 정의\n","class PsyExOnlyPostStream(nn.Module):\n","    def __init__(self, model_name, num_labels, num_heads=8, num_trans_layers=6, max_posts=64, freeze=False, pool_type=\"first\") -> None:\n","        super().__init__()\n","        self.model_name = model_name\n","        self.num_heads = num_heads\n","        self.num_labels = num_labels\n","        self.num_trans_layers = num_trans_layers\n","        self.pool_type = pool_type\n","        self.post_encoder = AutoModel.from_pretrained(model_name)\n","        if freeze:\n","            for name, param in self.post_encoder.named_parameters():\n","                param.requires_grad = False\n","        self.hidden_dim = self.post_encoder.config.hidden_size\n","        self.max_posts = max_posts\n","        self.pos_emb = nn.Parameter(torch.Tensor(max_posts, self.hidden_dim))\n","        nn.init.xavier_uniform_(self.pos_emb)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=self.hidden_dim, dim_feedforward=self.hidden_dim, nhead=num_heads, activation='gelu')\n","        self.user_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_trans_layers)\n","        self.attn_ff = nn.ModuleList([nn.Linear(self.hidden_dim, 1) for _ in range(self.num_labels)])\n","        self.dropout = nn.Dropout(self.post_encoder.config.hidden_dropout_prob)\n","        self.clf = nn.ModuleList([nn.Linear(self.hidden_dim, 1) for _ in range(self.num_labels)])\n","\n","    def forward(self, input_ids, attention_mask):\n","        \"\"\"\n","        Args:\n","            batch: A dictionary containing the following keys:\n","                - input_ids: Tensor of shape [batch_size, seq_len]\n","                - attention_mask: Tensor of shape [batch_size, seq_len]\n","                - token_type_ids: Tensor of shape [batch_size, seq_len] (optional)\n","        Returns:\n","            logits: Tensor of shape [batch_size, num_labels]\n","        \"\"\"\n","        input_ids = input_ids  # [batch_size, seq_len]\n","        attention_mask = attention_mask  # [batch_size, seq_len]\n","        #token_type_ids = batch.get(\"token_type_ids\", None)  # Optional\n","\n","        # BERT Encoder\n","        post_outputs = self.post_encoder(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","            #token_type_ids=token_type_ids\n","        )\n","\n","        # Pooling 처리\n","        if self.pool_type == \"first\":\n","            x = post_outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_dim]\n","        elif self.pool_type == 'mean':\n","            x = self.mean_pooling(post_outputs.last_hidden_state, attention_mask)  # [batch_size, hidden_dim]\n","\n","        # Transformer Encoder\n","        x = x + self.pos_emb[:x.size(0), :]  # Add positional embeddings\n","        x = self.user_encoder(x.unsqueeze(1)).squeeze(1)  # [batch_size, hidden_dim]\n","\n","        # Attention & Classification\n","        logits = []\n","        for i in range(self.num_labels):\n","            logits.append(self.clf[i](x))  # [batch_size, 1]\n","        logits = torch.cat(logits, dim=1)  # [batch_size, num_labels]\n","\n","        return logits\n","\n","    def mean_pooling(self, last_hidden_state, attention_mask):\n","        \"\"\"\n","        Mean pooling using attention mask.\n","        Args:\n","            last_hidden_state: [batch_size, seq_len, hidden_dim]\n","            attention_mask: [batch_size, seq_len]\n","        Returns:\n","            Pooled output: [batch_size, hidden_dim]\n","        \"\"\"\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        return sum_embeddings / sum_mask"],"metadata":{"id":"JzRrwc6sQV-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertModel\n","\n","class IntegratedModelWithBERT(nn.Module):\n","    def __init__(self, bert_model_name, embedding_dim, symptom_dim, num_labels, filter_num=64, filter_sizes=[1, 3, 5], max_pooling_k=1, dropout=0.2):\n","        super().__init__()\n","        self.embedding_dim = embedding_dim\n","        self.symptom_dim = symptom_dim\n","        self.num_labels = num_labels\n","\n","        # BERT 모델 초기화\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","        self.bert_hidden_size = self.bert.config.hidden_size  # BERT의 출력 크기\n","\n","        # CNN Layers\n","        self.filter_num = filter_num\n","        self.filter_sizes = filter_sizes\n","        self.hidden_size = len(filter_sizes) * filter_num\n","        self.convs = nn.ModuleList([nn.Conv1d(2, filter_num, size) for size in filter_sizes])  # input_channels=2\n","\n","        self.max_pooling_k = max_pooling_k\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(self.hidden_size + self.bert_hidden_size, num_labels)  # BERT 출력과 CNN 출력 결합\n","\n","    def forward(self, factor_input_ids, factor_attention_mask, symptoms):\n","        # BERT 출력 계산\n","        bert_outputs = self.bert(input_ids=factor_input_ids, attention_mask=factor_attention_mask)\n","        bert_pooled_output = bert_outputs.pooler_output  # (batch_size, bert_hidden_size)\n","\n","        # 차원 확장: factor_input_ids에 마지막 차원 추가\n","        factor_input_ids = factor_input_ids.unsqueeze(-1).float()  # (batch_size, seq_len, 1)\n","\n","        # Pooling으로 차원 축소: seq_len → symptom_dim\n","        factors_reduced = F.adaptive_avg_pool1d(factor_input_ids.transpose(1, 2), output_size=self.symptom_dim).transpose(1, 2)  # (batch_size, symptom_dim, 1)\n","\n","        # 증상 데이터 차원 맞춤\n","        input_seqs = torch.cat([symptoms, factors_reduced], dim=2)  # (batch_size, symptom_dim, 2)\n","\n","        # Conv1d 입력 차원 변환\n","        input_seqs = input_seqs.transpose(1, 2)  # (batch_size, 2, symptom_dim)\n","\n","        # CNN 처리\n","        x = [F.relu(conv(input_seqs)) for conv in self.convs]\n","        x = [self.kmax_pooling(item, self.max_pooling_k).mean(2) for item in x]\n","        x = torch.cat(x, 1)  # (batch_size, hidden_size)\n","        x = self.dropout(x)\n","\n","        # BERT 출력과 CNN 출력을 결합\n","        combined = torch.cat([bert_pooled_output, x], dim=1)  # (batch_size, hidden_size + bert_hidden_size)\n","\n","        # Fully Connected Layer for prediction\n","        logits = self.fc(combined)  # (batch_size, num_labels)\n","        return logits\n","\n","    def kmax_pooling(self, x, k):\n","        if x.dim() < 3:\n","            raise ValueError(f\"Input to kmax_pooling must have at least 3 dimensions, but got {x.shape}\")\n","        return x.sort(dim=2, descending=True)[0][:, :, :k]\n"],"metadata":{"id":"KHAnciwcQal8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 정의\n","class Symp(nn.Module):\n","    def __init__(self, input_dim, filter_num=128, filter_sizes=[1], dropout=0.2, num_labels=16, k_max_pooling=1):\n","        super(Symp, self).__init__()\n","        self.filter_num = filter_num\n","        self.filter_sizes = filter_sizes\n","        self.hidden_size = len(filter_sizes) * filter_num\n","        self.convs = nn.ModuleList([nn.Conv1d(input_dim, filter_num, size) for size in filter_sizes])\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(self.hidden_size, num_labels)\n","        self.k_max_pooling = k_max_pooling\n","\n","    def forward(self, input_seqs):\n","        # Conv1d 적용 및 활성화 함수\n","        x = [F.relu(conv(input_seqs)) for conv in self.convs]\n","\n","        # k-max pooling 적용\n","        x = [self.kmax_pooling(item, self.k_max_pooling) for item in x]\n","\n","        # 평균값 계산 및 결합\n","        x = [item.mean(dim=2) for item in x]  # [batch_size, filter_num]\n","        x = torch.cat(x, 1)  # [batch_size, len(filter_sizes) * filter_num]\n","\n","        # 드롭아웃 및 최종 출력\n","        x = self.dropout(x)\n","        logits = self.fc(x)  # [batch_size, num_labels]\n","        return torch.sigmoid(logits)  # Sigmoid 활성화 함수 적용\n","\n","\n","    def kmax_pooling(self, x, k):\n","        return x.sort(dim=2, descending=True)[0][:, :, :k]"],"metadata":{"id":"dYHU9C4tQb1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 정의\n","class SympWithFactor(nn.Module):\n","    def __init__(self, input_dim, num_factors, filter_num=64, filter_sizes=[1], dropout=0.2, num_labels=16, max_pooling_k=1):\n","        super(SympWithFactor, self).__init__()\n","        self.filter_num = filter_num\n","        self.filter_sizes = filter_sizes\n","        self.hidden_size = len(filter_sizes) * filter_num\n","        self.max_pooling_k = max_pooling_k\n","        self.convs = nn.ModuleList([nn.Conv1d(16, filter_num, size) for size in filter_sizes]) #num_factors * 2->16\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(self.hidden_size, num_labels)\n","\n","    def forward(self, symptoms, factors):\n","        # symptoms를 축소하여 factors와 동일한 크기로 조정\n","        symptoms_reduced = self.kmax_pooling(symptoms.unsqueeze(1), k=factors.size(1)).squeeze(1)  # [batch_size, num_factors]\n","\n","        # 축소된 symptoms와 factors를 결합\n","        input_seqs = torch.cat([symptoms_reduced, factors], dim=1).unsqueeze(1)  # [batch_size, 1, num_factors * 2]\n","\n","        # Conv1d에 맞게 차원 전환\n","        input_seqs = input_seqs.transpose(1, 2)  # [batch_size, num_factors * 2, 1]\n","        x = [F.relu(conv(input_seqs)) for conv in self.convs]\n","        x = [self.kmax_pooling(item, self.max_pooling_k).mean(2) for item in x]\n","        x = torch.cat(x, 1)\n","        x = self.dropout(x)\n","        logits = self.fc(x)\n","        return logits\n","\n","    def kmax_pooling(self, x, k):\n","        return x.sort(dim=2, descending=True)[0][:, :, :k]"],"metadata":{"id":"OeBWBz6WQdC5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CURE(nn.Module):\n","    def __init__(self, target_models, target_uncertainty, predictors, hidden_dim, num_labels, num_models, dropout_ratio, freeze=True):\n","        super(CURE, self).__init__()\n","        # Hyper-parameters\n","        self.num_labels = num_labels\n","        self.num_uncertainty = len(target_uncertainty)\n","        self.hidden_dim = hidden_dim\n","        self.num_models = len(target_models)\n","        # Model predictions:: sub-models\n","        self.target_models_name = target_models\n","        self.target_uncertainty = target_uncertainty\n","        self.predictors = predictors\n","        # Uncertainty-aware decision fusion layers\n","        self.linear1 = nn.Linear(112, self.hidden_dim)\n","        #self.linear1 = nn.Linear(self.num_models * self.num_labels + self.num_uncertainty, self.hidden_dim)\n","        self.linear2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n","        self.logits = nn.Linear(self.hidden_dim, self.num_labels)\n","        self.dropout = nn.Dropout(dropout_ratio)\n","        # Optional: Freeze sub-models\n","        self.reset_parameters()\n","        if freeze:\n","            for model in self.predictors:\n","                for name, param in model.named_parameters():\n","                    param.requires_grad = False\n","\n","    def reset_parameters(self): # Initialize weights\n","        nn.init.xavier_uniform_(self.linear1.weight)\n","        nn.init.xavier_uniform_(self.linear2.weight)\n","        nn.init.xavier_uniform_(self.logits.weight)\n","\n","    def make_prediction(self, model_type, predictor, input_data):\n","        if model_type in [\"bertq\"]:\n","            input_ids, attention_mask = input_data\n","            logits = predictor(input_ids, attention_mask)\n","            #print(\"bertq OK\")\n","            return torch.sigmoid(logits)\n","        if model_type in [\"bertc\"]:\n","            input_ids, attention_mask, symptom = input_data\n","            logits = predictor(input_ids, attention_mask, symptom)\n","            #print(\"bertc OK\")\n","            return torch.sigmoid(logits)\n","        if model_type in [\"sympc\"]:\n","            symptom, factor = input_data\n","            logits = predictor(symptom, factor)\n","            #print(\"sympc OK\")\n","            return torch.sigmoid(logits)\n","        else:  # \"symp\"\n","            logits = predictor(input_data)\n","            #print(\"sympp OK\")\n","            return torch.sigmoid(logits)\n","\n","    def forward(self, text_input_ids, text_attention_mask, factor_input_ids,\n","                factor_attention_mask, symptom, factors, symptom_uncertainty):\n","\n","        '''print(f\"text_input_ids: {text_input_ids.shape}\")\n","        print(f\"text_attention_mask: {text_attention_mask.shape}\")\n","        print(f\"factor_input_ids: {factor_input_ids.shape}\")\n","        print(f\"factor_attention_mask: {factor_attention_mask.shape}\")\n","        print(f\"symptom: {symptom.shape}\")\n","        print(f\"factors: {factors.shape}\")\n","        print(f\"symptom_uncertainty: {symptom_uncertainty.shape}\")'''\n","\n","        # Setting variables\n","        #swfactor = torch.cat([symptom, factors], dim=1).unsqueeze(2)\n","        #symptom = symptom.unsqueeze(2).squeeze(0)\n","        uncertainties = []\n","        pred_logits = []\n","\n","        # Model predictions\n","        with torch.no_grad():\n","            for model_type, predictor in zip(self.target_models_name, self.predictors):\n","                if model_type == \"bertq\":\n","                    input_data = (text_input_ids, text_attention_mask)\n","                elif model_type == \"bertc\":\n","                    input_data = (factor_input_ids, factor_attention_mask, symptom.unsqueeze(2).squeeze(0))\n","                elif model_type == \"symp\":\n","                    input_data = symptom.unsqueeze(2).squeeze(0)\n","                elif model_type == \"sympc\":\n","                    input_data = (symptom, factors)  # 기존 코드랑 다름??\n","                else:\n","                    raise ValueError(\"Invalid Model Type\")\n","\n","                pred_logit = self.make_prediction(model_type, predictor, input_data)\n","                pred_logits.append(pred_logit)\n","\n","        # Uncertainty-aware decision fusion\n","        # symptom_uncertainty는 외부에서 받은 불확실성\n","        uncertainties.append(symptom_uncertainty.unsqueeze(1))\n","\n","        '''# BertMultiLabelClassificationWithSNGP에서 얻은 불확실성\n","        for predictor in self.predictors:\n","            if isinstance(predictor, BertMultiLabelClassificationWithSNGP):\n","                # SNGP 모델에서 추출한 불확실성 값 받기\n","                mean_uncertainty = predictor.uncertainty  # 모델에서 추출한 불확실성 값\n","                uncertainties.append(mean_uncertainty.unsqueeze(1))'''\n","\n","        uncertainties = torch.cat(uncertainties, dim=1)  # [batch size, num_uncertainty]\n","\n","        stacked_all = torch.cat(pred_logits, dim=1)  # [batch_size, num_labels * num_models]\n","        cat = torch.cat([uncertainties.squeeze(1), stacked_all], dim=1)  # [batch_size, (num_labels * num_models) + num_uncertainty]\n","\n","        hidden = self.linear1(cat)\n","        hidden = F.elu(self.dropout(hidden))\n","        hidden = self.linear2(hidden)\n","        hidden = F.elu(self.dropout(hidden))\n","        logits = self.logits(hidden)\n","\n","        return logits"],"metadata":{"id":"5uVbUrTVQHrg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","# CUDA 장치에서 로드\n","with open('1205_model/bert_post.pkl', 'rb') as f:\n","    BERT_post = torch.load(f)\n","\n","with open('최종모델/bert_context_최종.pkl', 'rb') as f:\n","    BERT_context = torch.load(f)\n","\n","with open('최종모델/symp_symp_최종.pkl', 'rb') as f:\n","    SYMP_symp = torch.load(f)\n","\n","with open('최종모델/symp_context_최종.pkl', 'rb') as f:\n","    SYMP_context = torch.load(f)\n","\n","with open('최종모델/cure_최종.pkl', 'rb') as f:\n","    cure = torch.load(f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EUr3C1VKQf06","outputId":"32cbaaa0-1dcd-4c19-9fa4-78f3e9e964da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-15-859cea5787a6>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  BERT_post = torch.load(f)\n","<ipython-input-15-859cea5787a6>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  BERT_context = torch.load(f)\n","<ipython-input-15-859cea5787a6>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  SYMP_symp = torch.load(f)\n","<ipython-input-15-859cea5787a6>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  SYMP_context = torch.load(f)\n","<ipython-input-15-859cea5787a6>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  cure = torch.load(f)\n"]}]},{"cell_type":"markdown","source":["##CURE"],"metadata":{"id":"YpSb2oOtQG89"}},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, df, tokenizer, mean_uncertainty, max_length=128, batch_size=64):\n","        self.tokenizer = tokenizer\n","        self.mean_uncertainty = mean_uncertainty\n","        self.max_length = max_length\n","        self.batch_size = batch_size\n","\n","        # 복제 로직: 데이터를 batch_size에 맞게 복제\n","        self.df = pd.concat([df] * ((batch_size + len(df) - 1) // len(df)), ignore_index=True)\n","        self.mean_uncertainty = mean_uncertainty * ((batch_size + len(mean_uncertainty) - 1) // len(mean_uncertainty))\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        # 텍스트 필드 가져오기 및 문자열 변환\n","        filtered_sentences = str(row['filtered_sentences'])\n","        final_symptoms = str(row['final_symptoms'])\n","\n","        # Evidence 필드 처리\n","        evidence_fields = [\n","            str(evidence) if not pd.isna(evidence) else \"\"  # NaN을 빈 문자열로 대체\n","            for evidence in [\n","                row['cause_evidence'], row['freq_evidence'], row['duration_evidence'],\n","                row['age_evidence'], row['social_evidence'], row['academic_evidence'],\n","                row['occupational_evidence'], row['life-threatening_evidence']\n","            ]\n","        ]\n","\n","        # Evidence 토크나이징\n","        evidence_input_ids_list = [\n","            self.tokenizer.encode(evidence, add_special_tokens=True, truncation=True,\n","                                  max_length=self.max_length, padding='max_length')\n","            for evidence in evidence_fields\n","        ]\n","\n","        # Attention Mask 생성\n","        def create_attention_mask(input_ids):\n","            return [1 if token_id != 0 else 0 for token_id in input_ids]\n","\n","        evidence_attention_mask_list = [create_attention_mask(ids) for ids in evidence_input_ids_list]\n","\n","        # Factor 증거 결합\n","        factor_input_ids, factor_attention_mask = [], []\n","        for input_ids, attn_mask in zip(evidence_input_ids_list, evidence_attention_mask_list):\n","            factor_input_ids.extend(input_ids)\n","            factor_attention_mask.extend(attn_mask)\n","\n","        factor_input_ids = factor_input_ids[:self.max_length]\n","        factor_attention_mask = factor_attention_mask[:self.max_length]\n","\n","        # 텍스트 토크나이징\n","        text_inputs = self.tokenizer(\n","            filtered_sentences,\n","            padding='max_length',\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","\n","        # 질병 레이블 텐서 생성\n","        symptom_list = ['무증상', '짜증이 잘 남', '두려움과 공포', '호흡곤란', '폭식', '답답함', '초조하고 안절부절 못함', '언어 능력 감소',\n","                        '피해망상', '불면', '성에 대한 과도한 관심', '우울함', '강박사고', '음식 섭취 제한', '자살사고', '다리 불편감', '기억력 저하',\n","                        '체중과 체형에 대한 집착', '무기력', '주의산만', '긴장', '감정 기복', '환각', '수면 무호흡증', '환청', '수면 과다', '충동성',\n","                        '피로', '메스꺼움', '어지러움', '눈물 흘림', '예민함', '극단적인 생각', '반복행동', '구토와 약물', '수면문제', '불안함',\n","                        '자극적인 행위에 몰입', '통제력 부족', '분노 조절 문제', '발작', '두통', '가슴 두근거림', '장시간 지속되는 이유 없는 불안감',\n","                        '집중력 저하', '일상생활 능력의 손상', '식욕감소', '걱정이 많아짐']\n","        symptom_labels = final_symptoms\n","        symptom_name_to_index = {name: idx for idx, name in enumerate(symptom_list)}\n","\n","        symptom_tensor = torch.zeros(len(symptom_list))\n","        for label in symptom_labels:\n","            if label in symptom_name_to_index:\n","                symptom_tensor[symptom_name_to_index[label]] = 1\n","\n","        # Factors 텐서 생성\n","        factors = {\n","            'cause': row['cause'],\n","            'frequency': row['frequency'],\n","            'duration': row['duration'],\n","            'age': row['age'],\n","            'social': row['social'],\n","            'academic': row['academic'],\n","            'occupational': row['occupational'],\n","            'life-threatening': row['life-threatening']\n","        }\n","        factors_tensor = torch.tensor([factors[key] for key in factors], dtype=torch.float32)\n","\n","        # Uncertainty 값 가져오기\n","        uncertainty = self.mean_uncertainty[min(idx, len(self.mean_uncertainty) - 1)]\n","\n","        return {\n","            'sentences_input_ids': text_inputs['input_ids'].squeeze(0),\n","            'sentences_attention_mask': text_inputs['attention_mask'].squeeze(0),\n","            'final_symptoms': symptom_tensor,\n","            'factor_input_ids': torch.tensor(factor_input_ids, dtype=torch.long),\n","            'factor_attention_mask': torch.tensor(factor_attention_mask, dtype=torch.long),\n","            'factors': factors_tensor,\n","            'symptom_uncertainty': torch.tensor(uncertainty, dtype=torch.float)\n","        }\n"],"metadata":{"id":"TlU2cR7HQ58F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","tokenizer=AutoTokenizer.from_pretrained('klue/bert-base')\n","\n","# CustomDataset 객체 생성\n","dataset = CustomDataset(df, tokenizer, mean_uncertainty)\n","\n","# DataLoader 생성\n","dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n","\n","for batch in dataloader:\n","    print(f\"sentences_input_ids shape: {batch['sentences_input_ids'].shape}\")\n","    print(f\"sentences_attention_mask shape: {batch['sentences_attention_mask'].shape}\")\n","    print(f\"final_symptoms shape: {batch['final_symptoms'].shape}\")\n","    print(f\"factor_input_ids shape: {batch['factor_input_ids'].shape}\")\n","    print(f\"factor_attention_mask shape: {batch['factor_attention_mask'].shape}\")\n","    print(f\"factors shape: {batch['factors'].shape}\")\n","    print(f\"symptom_uncertainty shape: {batch['symptom_uncertainty'].shape}\")\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g__uG2gKku_v","outputId":"9e4e7cf9-5028-43ad-fd2d-cf451651cdae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","<ipython-input-18-b956d9827d72>:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'symptom_uncertainty': torch.tensor(uncertainty, dtype=torch.float)\n"]},{"output_type":"stream","name":"stdout","text":["sentences_input_ids shape: torch.Size([64, 128])\n","sentences_attention_mask shape: torch.Size([64, 128])\n","final_symptoms shape: torch.Size([64, 48])\n","factor_input_ids shape: torch.Size([64, 128])\n","factor_attention_mask shape: torch.Size([64, 128])\n","factors shape: torch.Size([64, 8])\n","symptom_uncertainty shape: torch.Size([64, 48])\n"]}]},{"cell_type":"code","source":["# 모델 검증\n","average_threshold = 0.4  # 모델 출력의 평균 확률값을 기준으로 설정\n","\n","predictions, true_labels = [], []  # 예측값과 실제값 저장 리스트\n","\n","with torch.no_grad():  # 그래디언트 계산 비활성화\n","    for batch in dataloader:#train_loader:\n","        # 텍스트 (입력 시퀀스)\n","        sentences_input_ids = batch[\"sentences_input_ids\"].to(device)\n","        sentences_attention_mask = batch[\"sentences_attention_mask\"].to(device)\n","        factor_input_ids = batch[\"factor_input_ids\"].to(device)\n","        factor_attention_mask = batch[\"factor_attention_mask\"].to(device)\n","        symptom = batch[\"final_symptoms\"].to(device)  # 증상(symptom) 텍스트\n","        factors = batch[\"factors\"].to(device)  # factor는 여러 증거 텍스트를 결합한 것\n","        symptom_uncertainty = batch[\"symptom_uncertainty\"].to(device)\n","\n","        # 모델 예측\n","        logits = cure(\n","            text_input_ids=sentences_input_ids,\n","            text_attention_mask=sentences_attention_mask,\n","            factor_input_ids=factor_input_ids,\n","            factor_attention_mask=factor_attention_mask,\n","            symptom=symptom,\n","            factors=factors,\n","            symptom_uncertainty=symptom_uncertainty\n","        )\n","        probabilities = torch.sigmoid(logits)  # 로짓 값을 확률로 변환\n","\n","        # 임계값을 기준으로 이진화\n","        predicted_labels = (probabilities > average_threshold).int()\n","        predictions.append(predicted_labels)\n","\n","print(predictions[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOCkSLRJlbhF","outputId":"7c96c44b-20a4-4002-abf3-cd9544d3028b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-18-b956d9827d72>:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'symptom_uncertainty': torch.tensor(uncertainty, dtype=torch.float)\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[0, 0, 1,  ..., 0, 0, 0],\n","        [0, 0, 1,  ..., 0, 0, 0],\n","        [0, 0, 1,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 1,  ..., 0, 0, 0],\n","        [0, 0, 1,  ..., 0, 0, 0],\n","        [0, 0, 1,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)\n"]}]},{"cell_type":"code","source":["predictions[0][0]\n","\n","#임계값 평균으로 했을 때: 수면장애 섭식장애 스트레스 불안장애 ADHD 공황장애 틱장애 조현병\n","#임계값 더 올려서 0.4로 했을 때: 섭식장애"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DMXoxjNfu0lZ","outputId":"e11f256b-6efa-4e8a-f520-31cee641a8a4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n","       dtype=torch.int32)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["#[\"수면장애\", \"우울증\", \"섭식장애\", \"스트레스\", \"불안장애\", \"ADHD\", \"Non-disease\", \"공황장애\",\n","                   \"강박장애\", \"사회 불안장애\", \"중독(의존)\", \"하지 불안 증후군\", \"치매\", \"틱장애\", \"조현병\", \"양극성장애\"]"],"metadata":{"id":"b8cpJ0mdvFUF"},"execution_count":null,"outputs":[]}]}